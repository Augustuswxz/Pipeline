import streamlit as st
import os
import time
from langchain_core.messages import HumanMessage, AIMessage
import re
import streamlit.components.v1 as components
import sys
import builtins
import io

# =============================================================
# ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šå¼ºåŠ› Print æ•è·å™¨ (Hook builtins.print)
# =============================================================
class AggressivePrintCapture:
    """
    è¿™æ˜¯ä¸€ä¸ªå¼ºåŠ›æ•è·å™¨ã€‚
    å®ƒä¸ä¾èµ– sys.stdout é‡å®šå‘ï¼Œè€Œæ˜¯ç›´æ¥ Hook æ‰ Python çš„ print å‡½æ•°ã€‚
    """
    def __init__(self):
        self.log_buffer = []
        self.original_print = builtins.print
        self.log_placeholder = None

    def set_placeholder(self, placeholder):
        self.log_placeholder = placeholder

    def _hooked_print(self, *args, **kwargs):
        # 1. æ„å»ºè¾“å‡ºå­—ç¬¦ä¸²
        sep = kwargs.get('sep', ' ')
        end = kwargs.get('end', '\n')
        text = sep.join(map(str, args)) + end

        # 2. ğŸ”¥ å¼ºåˆ¶å†™å…¥ VS Code çœŸå®ç»ˆç«¯ (ç»•è¿‡ Streamlit å°è£…)
        try:
            sys.__stdout__.write(text)
            sys.__stdout__.flush()
        except Exception:
            pass

        # 3. è®°å½•åˆ°å†…å­˜ buffer
        self.log_buffer.append(text)

        # 4. (å¯é€‰) å®æ—¶æ˜¾ç¤ºåœ¨ç½‘é¡µé¡¶éƒ¨ï¼Œäº§ç”Ÿâ€œåˆ·å±â€æ•ˆæœ
        if self.log_placeholder:
            # åªæ˜¾ç¤ºæœ€è¿‘çš„ 5 è¡Œï¼Œé¿å…å¤ªé•¿
            recent_logs = "".join(self.log_buffer[-5:])
            self.log_placeholder.code(recent_logs, language="text")

    def get_all_logs(self):
        return "".join(self.log_buffer)

    def __enter__(self):
        builtins.print = self._hooked_print
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        builtins.print = self.original_print


# =============================================================
# Mermaid æ¸²æŸ“å‡½æ•°
# =============================================================
def render_mermaid_html(mermaid_code, height=300):
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    </head>
    <body>
        <div class="mermaid">
            {mermaid_code}
        </div>
        <script>
            mermaid.initialize({{
                startOnLoad: true,
                theme: 'default',
                securityLevel: 'loose',
            }});
        </script>
    </body>
    </html>
    """
    components.html(html_content, height=height, scrolling=False)

def render_flowchart_stepwise(container, mode: str, interval=0.8):
    clean_steps = [
        "è¡¨æ ¼ç»“æ„è§£æ",
        "çŸ¥è¯†åº“è‡ªåŠ¨è¯†åˆ«",
        "ç»“æ„æ¨¡æ¿æ˜ å°„",
        "å•ä½è½¬æ¢åŠé‡çº²æ ‡å‡†åŒ–",
        "ç»“æ„ç»Ÿä¸€åŠè¯­ä¹‰ç»Ÿä¸€çš„æ ‡å‡†åŒ–æ•°æ®",
    ]

    align_steps = [
        "æ ‡å‡†åŒ–æ•°æ®",
        "ä¸šåŠ¡åœºæ™¯åˆ†æ",
        "é”šç‚¹è¯†åˆ«",
        "ç„Šç¼/ä¸‰æ¡©åº¦é‡",
        "é”šç‚¹å¯¹é½",
        "ç¼ºé™·åˆ†æä¸åº¦é‡",
        "ç¼ºé™·å¯¹é½",
        "å¤šæºåˆå¹¶æ•°æ®",
    ]

    STEPS = clean_steps if mode == "clean" else align_steps
    TITLE = "æ¸…æ´—æµç¨‹" if mode == "clean" else "å¯¹é½æµç¨‹"
    
    placeholder = container.empty()
    
    for i in range(1, len(STEPS) + 1):
        current_steps = STEPS[:i]
        lines = ["flowchart LR"]
        lines.append(f'    subgraph {TITLE} ["ğŸš€ {TITLE}"]')
        lines.append("    direction LR")
        
        for idx, s in enumerate(current_steps):
            node_id = f"Node{idx}"
            if idx == i - 1:
                lines.append(f'        {node_id}["âœ¨ {s}"]:::active')
            else:
                lines.append(f'        {node_id}["{s}"]')
        
        for idx in range(len(current_steps) - 1):
            lines.append(f"        Node{idx} --> Node{idx+1}")
            
        lines.append("    end")
        lines.append("    classDef active fill:#f96,stroke:#333,stroke-width:2px,color:white;")
        
        final_code = "\n".join(lines)
        with placeholder:
            render_mermaid_html(final_code, height=250)
        time.sleep(interval)

# =============================================================
# æ¸²æŸ“èŠ‚ç‚¹ä¿¡æ¯
# =============================================================
def render_step_details(container, value, node_name):
    container.markdown(f"#### âš™ï¸ æ­£åœ¨æ‰§è¡ŒèŠ‚ç‚¹: `{node_name}`")

    if isinstance(value, dict) and "memory" in value and value["memory"]:
        with container.expander("ğŸ§  è®°å¿†æ›´æ–°", expanded=False):
            st.json(value["memory"])

    if isinstance(value, dict) and "messages" in value and value["messages"]:
        last_msg = value["messages"][-1]
        content = getattr(last_msg, "content", str(last_msg))

        if isinstance(last_msg, AIMessage):
            container.info(f"ğŸ¤– **èŠ‚ç‚¹è¾“å‡º**:\n{content}")
        else:
            container.write(f"ğŸ‘¤ **è¾“å…¥**:\n{content}")
    
    container.divider()

# =============================================================
# ğŸ”¥ æ–°å¢å·¥å…·å‡½æ•°ï¼šæ¸²æŸ“æ¶ˆæ¯å†…å®¹åŠä¸‹è½½æŒ‰é’®
# =============================================================
def render_message_content(content, unique_key_prefix):
    """
    æ¸²æŸ“æ¶ˆæ¯æ–‡æœ¬ï¼Œå¹¶æ£€æµ‹æ˜¯å¦æœ‰æ–‡ä»¶ä¸‹è½½æ ‡è®° [FILE:xxx]ã€‚
    å¦‚æœæœ‰ï¼Œåˆ™æ¸²æŸ“ä¸‹è½½æŒ‰é’®ã€‚
    """
    st.markdown(content)
    
    # æ£€æµ‹æ–‡ä»¶æ ‡è®°
    generated_files = re.findall(r"\[FILE:(.*?)\]", content)
    
    if generated_files:
        st.markdown("---") # åˆ†å‰²çº¿
        st.caption("ğŸ“ æ£€æµ‹åˆ°ç”Ÿæˆæ–‡ä»¶ï¼š")
        
        for idx, filename in enumerate(generated_files):
            filepath = os.path.join("GeneratedFiles", filename)
            
            # ç¡®ä¿æ¯ä¸ªæŒ‰é’®æœ‰å”¯ä¸€çš„ keyï¼Œå¦åˆ™ Streamlit ä¼šæŠ¥é”™
            btn_key = f"dl_{unique_key_prefix}_{idx}_{filename}"
            
            if os.path.exists(filepath):
                with open(filepath, "rb") as f:
                    st.download_button(
                        label=f"â¬‡ï¸ ä¸‹è½½ {filename}",
                        data=f.read(),
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key=btn_key
                    )
            else:
                st.warning(f"âš ï¸ æ–‡ä»¶å·²è¿‡æœŸæˆ–ä¸å­˜åœ¨ï¼š{filename}")

# =============================================================
# å¯¼å…¥å›¾ï¼ˆGraphï¼‰
# =============================================================
try:
    from graph import build_graph
except:
    st.error("âŒ æ‰¾ä¸åˆ° graph.py / build_graphï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ç»“æ„")
    st.stop()

# =============================================================
# Streamlit é¡µé¢è®¾ç½®
# =============================================================
st.set_page_config(page_title="AI æ•°æ®å¤„ç†åŠ©æ‰‹", layout="wide")
st.title("ğŸ¤– AI æ•°æ®å¤„ç†åŠ©æ‰‹ï¼ˆLangGraph + Streamlitï¼‰")

UPLOAD_DIR = "UploadedFiles"
GENERATED_DIR = "GeneratedFiles"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(GENERATED_DIR, exist_ok=True)

if "messages" not in st.session_state:
    st.session_state.messages = []

if "thread_id" not in st.session_state:
    st.session_state.thread_id = "user_streamlit_001"

if "graph" not in st.session_state:
    st.session_state.graph = build_graph()

config = {"configurable": {"thread_id": st.session_state.thread_id}}

# ä¸šåŠ¡åœºæ™¯
ALIGNMENT_TYPES = {
    "è‡ªåŠ¨/æ™ºèƒ½è¯†åˆ«": "auto",
    "å†…æ£€æµ‹å¯¹é½": "internal",
    "å¤–æ£€æµ‹å¯¹é½": "external",
    "å»ºè®¾æœŸå¯¹é½": "construction"
}

# =============================================================
# ä¾§è¾¹æ 
# =============================================================
with st.sidebar:
    st.header("ğŸ› ï¸ æ•°æ®ä»»åŠ¡é¢æ¿")

    def update_agent_memory(new_data_dict):
        try:
            current_state = st.session_state.graph.get_state(config)
            current_memory = current_state.values.get("memory", {}) if current_state.values else {}
            current_memory.update(new_data_dict)
            st.session_state.graph.update_state(config, {"memory": current_memory})
            st.toast(f"ğŸ§  è®°å¿†å·²æ›´æ–°: {new_data_dict}")
        except Exception as e:
            st.error(f"è®°å¿†åŒæ­¥å¤±è´¥: {e}")

    tab_clean, tab_align = st.tabs(["ğŸ§¹ æ•°æ®æ¸…æ´—", "ğŸ§© æ•°æ®å¯¹é½"])

    with tab_clean:
        st.caption("ä¸Šä¼ å•ä¸ªæ–‡ä»¶è¿›è¡Œæ ¼å¼æ¸…æ´—")
        clean_file = st.file_uploader("é€‰æ‹©æ–‡ä»¶", type=["xlsx", "xls"], key="clean_file")
        if clean_file:
            path = os.path.join(UPLOAD_DIR, clean_file.name)
            with open(path, "wb") as f:
                f.write(clean_file.getbuffer())
            update_agent_memory({"cleaning_target": clean_file.name})
            st.success(f"å·²å°±ç»ªï¼š{clean_file.name}")

    with tab_align:
        st.caption("ä¸Šä¼ ä¸¤ä¸ªæ–‡ä»¶è¿›è¡Œç¼ºé™·ä¸ç„Šç¼é”šç‚¹å¯¹é½")

        # é€‰æ‹©ä¸šåŠ¡åœºæ™¯
        # 1. æ”¹ä¸ºä¸‹æ‹‰æ¡† (Selectbox)
        selected_label = st.selectbox(
            "é€‰æ‹©æ•°æ®åœºæ™¯ (å¯é€‰):",
            options=list(ALIGNMENT_TYPES.keys()),
            index=0,
            key="scenario_selector" # åŠ ä¸Š key æ˜¯ä¸ªå¥½ä¹ æƒ¯
        )

        # 2. è·å–å¯¹åº”çš„å€¼ (å¦‚ "internal")
        scenario_context = ALIGNMENT_TYPES[selected_label]
        # 3. ã€æ ¸å¿ƒæ­¥éª¤ã€‘ç«‹åˆ»æ›´æ–°è®°å¿†
        # é€»è¾‘ï¼šå¦‚æœé€‰çš„æ˜¯"è‡ªåŠ¨"ï¼Œä¼ ç»™åç«¯ Noneï¼›å¦åˆ™ä¼ å…·ä½“çš„å€¼
        # è¿™æ ·åç«¯å¯ä»¥ç”¨ if ui_scenario: æ¥åˆ¤æ–­æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œ
        memory_value = scenario_context if scenario_context != "auto" else None
        # åªè¦é¡µé¢åˆ·æ–°ï¼ˆç”¨æˆ·åšäº†é€‰æ‹©ï¼‰ï¼Œè¿™é‡Œå°±ä¼šæ‰§è¡Œï¼Œå°†æ–°çŠ¶æ€åŒæ­¥ç»™ Agent
        update_agent_memory({"alignment_scenario": memory_value})

        f1 = st.file_uploader("åŸºå‡†æ–‡ä»¶ File 1", type=["xlsx", "xls"], key="align1")
        if f1:
            path = os.path.join(UPLOAD_DIR, f1.name)
            with open(path, "wb") as f:
                f.write(f1.getbuffer())
            update_agent_memory({"align_file1": f1.name})
            st.info(f"åŸºå‡†æ–‡ä»¶ï¼š{f1.name}")

        f2 = st.file_uploader("ç›®æ ‡æ–‡ä»¶ File 2", type=["xlsx", "xls"], key="align2")
        if f2:
            path = os.path.join(UPLOAD_DIR, f2.name)
            with open(path, "wb") as f:
                f.write(f2.getbuffer())
            update_agent_memory({"align_file2": f2.name})
            st.info(f"ç›®æ ‡æ–‡ä»¶ï¼š{f2.name}")


# =============================================================
# ä¸»èŠå¤©åŒºåŸŸï¼šæ¸²æŸ“å†å²å¯¹è¯
# =============================================================
for i, msg in enumerate(st.session_state.messages):
    with st.chat_message("user" if isinstance(msg, HumanMessage) else "assistant"):
        render_message_content(msg.content, unique_key_prefix=f"history_{i}")


# =============================================================
# å¤„ç†ç”¨æˆ·è¾“å…¥
# =============================================================
if user_input := st.chat_input("è¯·è¾“å…¥ä½ çš„æŒ‡ä»¤â€¦"):

    # 1. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    st.chat_message("user").write(user_input)
    st.session_state.messages.append(HumanMessage(content=user_input))

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        llm_response_text = ""
        
        capturer = AggressivePrintCapture()

        try:
            inputs = {"messages": [HumanMessage(content=user_input)]}
            mode = None
            if "æ¸…æ´—" in user_input: mode = "clean"
            if "å¯¹é½" in user_input: mode = "align"

            # 2. æ€ç»´å¯¼å›¾åŠ¨ç”»
            if mode:
                with st.expander("ğŸ“Š æ™ºèƒ½ä½“æ€ç»´è§„åˆ’ï¼ˆåŠ¨æ€ï¼‰", expanded=True):
                    render_flowchart_stepwise(st, mode)
            
            # 3. è¿‡ç¨‹æ—¥å¿—å®¹å™¨ (ä½äºæ€ç»´å¯¼å›¾ä¸‹æ–¹ï¼Œæœ€ç»ˆå›ç­”ä¸Šæ–¹)
            log_display_container = st.container()
            
            # 4. è¿è¡Œ Graph
            with st.status("ğŸ¤– AI æ­£åœ¨æ‰§è¡Œä»»åŠ¡...", expanded=True) as status_box:
                
                log_box = st.empty()
                capturer.set_placeholder(log_box)

                with capturer:
                    events = st.session_state.graph.stream(inputs, config=config)

                    for event in events:
                        for node_name, value in event.items():
                            if value is None: continue

                            render_step_details(status_box, value, node_name)

                            if isinstance(value, dict) and "messages" in value and value["messages"]:
                                last_msg = value["messages"][-1]
                                if isinstance(last_msg, AIMessage):
                                    llm_response_text = last_msg.content
                                    message_placeholder.markdown(llm_response_text + "â–Œ")
                
                status_box.update(label="âœ… å¤„ç†å®Œæˆ", state="complete", expanded=False)

            # ------------------------------------------------------------------
            # 5. ğŸ”¥ å¤„ç†æ—¥å¿—è¾“å‡º (åˆ†ç¦»æ¸²æŸ“å¹¶å»é™¤é‡å¤è¡Œ)
            # ------------------------------------------------------------------
            captured_text = capturer.get_all_logs()
            log_section_for_history = ""
            
            if captured_text.strip():
                log_lines = captured_text.strip().split('\n')
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç§»é™¤æœ€åå¯èƒ½é‡å¤çš„æ—¥å¿—è¡Œ (ä¾‹å¦‚ï¼šå¯¹é½å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°...)
                # æ£€æŸ¥æœ€åä¸€è¡Œæ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼ˆè¿™æ˜¯ LangChain å€¾å‘äºé‡å¤åˆ°æœ€ç»ˆè¾“å‡ºä¸­çš„å†…å®¹ï¼‰
                if 'å¯¹é½å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:' in log_lines[-1] and len(log_lines) > 1:
                    log_lines.pop()
                    
                captured_text_filtered = '\n'.join(log_lines) + '\n' if log_lines else ''

                # 5a. æ¸²æŸ“åˆ°ä¸´æ—¶å®¹å™¨ (æ»¡è¶³åœ¨å›ç­”ä¸Šæ–¹çš„è¦æ±‚)
                log_display_container.markdown("---")
                log_display_container.markdown("### ğŸ§¾ è¿‡ç¨‹æ—¥å¿— (Process Logs)")
                log_display_container.code(captured_text_filtered, language="text")

                # 5b. å‡†å¤‡å†å²è®°å½•çš„ä¿å­˜æ ¼å¼ (ä½¿ç”¨è¿‡æ»¤åçš„æ—¥å¿—)
                if captured_text_filtered.strip():
                    log_section_for_history = f"\n\n---\n**ğŸ§¾ è¿‡ç¨‹æ—¥å¿— (Process Logs):**\n```text\n{captured_text_filtered}\n```"
            
            # 6. ç»„åˆæœ€ç»ˆå†…å®¹ (LLMæ–‡æœ¬ + å†å²æ—¥å¿—)
            full_response_with_logs = llm_response_text + log_section_for_history

            # æ¸…ç©º streaming placeholder
            message_placeholder.empty()

            # 7. ä¿å­˜æ¶ˆæ¯åˆ° Session State
            st.session_state.messages.append(AIMessage(content=full_response_with_logs))
            
            # 8. ç«‹å³è°ƒç”¨è‡ªå®šä¹‰æ¸²æŸ“å‡½æ•°æ˜¾ç¤ºæœ€ç»ˆç»“æœå’ŒæŒ‰é’®
            render_message_content(full_response_with_logs, unique_key_prefix=f"current_{len(st.session_state.messages)}")

        except Exception as e:
            st.error(f"âŒ è¿è¡Œé”™è¯¯ï¼š{e}")
            import traceback
            st.code(traceback.format_exc())