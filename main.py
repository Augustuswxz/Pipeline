import streamlit as st
import os
import time
from langchain_core.messages import HumanMessage, AIMessage
import re
import streamlit.components.v1 as components
import sys
import builtins
import io
import uuid
from render import AggressivePrintCapture, render_mermaid_html, render_flowchart_stepwise, render_step_details, render_message_content
from graph import build_graph

# =============================================================
# Streamlit é¡µé¢è®¾ç½®
# =============================================================
st.set_page_config(page_title="AI æ•°æ®å¤„ç†åŠ©æ‰‹", layout="wide")
st.title("ğŸ¤– AI æ•°æ®å¤„ç†åŠ©æ‰‹ï¼ˆLangGraph + Streamlitï¼‰")

UPLOAD_DIR = "UploadedFiles"
GENERATED_DIR = "GeneratedFiles"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(GENERATED_DIR, exist_ok=True)

if "messages" not in st.session_state:
    st.session_state.messages = []

if "thread_id" not in st.session_state:
    st.session_state.thread_id = "user_streamlit_001"

if "graph" not in st.session_state:
    st.session_state.graph = build_graph()

# config = {"configurable": {"thread_id": st.session_state.thread_id}}

# ä¸šåŠ¡åœºæ™¯
ALIGNMENT_TYPES = {
    "è‡ªåŠ¨/æ™ºèƒ½è¯†åˆ«": "auto",
    "å†…æ£€æµ‹å¯¹é½": "internal",
    "å¤–æ£€æµ‹å¯¹é½": "external",
    "å»ºè®¾æœŸå¯¹é½": "construction"
}

# =============================================================
# ä¾§è¾¹æ 
# =============================================================
with st.sidebar:
    st.header("ğŸ› ï¸ æ•°æ®ä»»åŠ¡é¢æ¿")
    st.write(f"å½“å‰ä¼šè¯: `{st.session_state.thread_id}`")
    
    if st.button("ğŸ§¹ å¼€å¯æ–°å¯¹è¯", use_container_width=True):
        # A. è·å–æ—§è®°å¿†ï¼ˆä¿ç•™æ–‡ä»¶ï¼‰
        try:
            # æ³¨æ„ï¼šè¿™é‡Œè¦ä¸´æ—¶æ„å»ºä¸€ä¸ªæ—§çš„ config æ¥è¯»å–æ—§è®°å¿†
            old_config = {"configurable": {"thread_id": st.session_state.thread_id}}
            current_state = st.session_state.graph.get_state(old_config)
            saved_memory = current_state.values.get("memory", {})
        except:
            saved_memory = {}
            
        # B. ç”Ÿæˆæ–° ID
        new_thread_id = str(uuid.uuid4())[:8]
        st.session_state.thread_id = new_thread_id
        
        # C. åˆå§‹åŒ–æ–°çº¿ç¨‹ (å…³é”®ï¼šå†™å…¥è®°å¿†ï¼Œä½†ä¸å¸¦ä»»ä½• next çŠ¶æ€)
        new_config = {"configurable": {"thread_id": new_thread_id}}
        st.session_state.graph.update_state(
            new_config, 
            {"messages": [], "memory": saved_memory} # ä»…å†™å…¥è®°å¿†
        )
        
        # D. æ¸…ç©ºå‰ç«¯æ˜¾ç¤º
        st.session_state.messages = []
        
        # E. â˜…â˜…â˜… å¼ºåˆ¶ç«‹åˆ»é‡å¯è„šæœ¬ â˜…â˜…â˜…
        # è¿™ç¡®ä¿äº†ä¸‹é¢çš„ä»£ç ä¼šä½¿ç”¨æ–°çš„ ID é‡æ–°è¿è¡Œ
        st.rerun()

    # è®°å¿†æ›´æ–°å‡½æ•°
    def update_agent_memory(new_data_dict):
        # ğŸ”¥ ä¿®å¤æ ¸å¿ƒï¼šåœ¨å‡½æ•°å†…éƒ¨åŠ¨æ€æ„å»º configï¼Œç¡®ä¿å®ƒæ˜¯æœ€æ–°çš„ä¸”å·²å®šä¹‰çš„
        # ä¾èµ– st.session_state.thread_idï¼Œè¿™ä¸ªå˜é‡åœ¨ä»£ç é¡¶éƒ¨å·²ç»åˆå§‹åŒ–äº†ï¼Œæ‰€ä»¥æ˜¯å®‰å…¨çš„
        local_config = {"configurable": {"thread_id": st.session_state.thread_id}}

        try:
            current_state = st.session_state.graph.get_state(local_config)
            current_memory = current_state.values.get("memory", {}) if current_state.values else {}
            current_memory.update(new_data_dict)
            
            # ä½¿ç”¨ local_config æ›´æ–°çŠ¶æ€
            st.session_state.graph.update_state(local_config, {"memory": current_memory})
            st.toast(f"ğŸ§  è®°å¿†å·²æ›´æ–°: {new_data_dict}")
        except Exception as e:
            st.error(f"è®°å¿†åŒæ­¥å¤±è´¥: {e}")

    tab_clean, tab_align = st.tabs(["ğŸ§¹ æ•°æ®æ¸…æ´—", "ğŸ§© æ•°æ®å¯¹é½"])

    # æ•°æ®æ¸…æ´—æ 
    with tab_clean:
        st.caption("ä¸Šä¼ å•ä¸ªæ–‡ä»¶è¿›è¡Œæ ¼å¼æ¸…æ´—")
        clean_file = st.file_uploader("é€‰æ‹©æ–‡ä»¶", type=["xlsx", "xls"], key="clean_file")
        if clean_file:
            path = os.path.join(UPLOAD_DIR, clean_file.name)
            with open(path, "wb") as f:
                f.write(clean_file.getbuffer())
            update_agent_memory({"cleaning_target": clean_file.name})
            st.success(f"å·²å°±ç»ªï¼š{clean_file.name}")

    # æ•°æ®å¯¹é½æ 
    with tab_align:
        st.caption("ä¸Šä¼ ä¸¤ä¸ªæ–‡ä»¶è¿›è¡Œç¼ºé™·ä¸ç„Šç¼é”šç‚¹å¯¹é½")

        # é€‰æ‹©ä¸šåŠ¡åœºæ™¯
        # 1. æ”¹ä¸ºä¸‹æ‹‰æ¡† (Selectbox)
        selected_label = st.selectbox(
            "é€‰æ‹©æ•°æ®åœºæ™¯ (å¯é€‰):",
            options=list(ALIGNMENT_TYPES.keys()),
            index=0,
            key="scenario_selector" # åŠ ä¸Š key æ˜¯ä¸ªå¥½ä¹ æƒ¯
        )

        # 2. è·å–å¯¹åº”çš„å€¼ (å¦‚ "internal")
        scenario_context = ALIGNMENT_TYPES[selected_label]
        # 3. ã€æ ¸å¿ƒæ­¥éª¤ã€‘ç«‹åˆ»æ›´æ–°è®°å¿†
        # é€»è¾‘ï¼šå¦‚æœé€‰çš„æ˜¯"è‡ªåŠ¨"ï¼Œä¼ ç»™åç«¯ Noneï¼›å¦åˆ™ä¼ å…·ä½“çš„å€¼
        # è¿™æ ·åç«¯å¯ä»¥ç”¨ if ui_scenario: æ¥åˆ¤æ–­æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œ
        memory_value = scenario_context if scenario_context != "auto" else None
        # åªè¦é¡µé¢åˆ·æ–°ï¼ˆç”¨æˆ·åšäº†é€‰æ‹©ï¼‰ï¼Œè¿™é‡Œå°±ä¼šæ‰§è¡Œï¼Œå°†æ–°çŠ¶æ€åŒæ­¥ç»™ Agent
        update_agent_memory({"alignment_scenario": memory_value})

        f1 = st.file_uploader("åŸºå‡†æ–‡ä»¶ File 1", type=["xlsx", "xls"], key="align1")
        if f1:
            path = os.path.join(UPLOAD_DIR, f1.name)
            with open(path, "wb") as f:
                f.write(f1.getbuffer())
            update_agent_memory({"align_file1": f1.name})
            st.info(f"åŸºå‡†æ–‡ä»¶ï¼š{f1.name}")

        f2 = st.file_uploader("ç›®æ ‡æ–‡ä»¶ File 2", type=["xlsx", "xls"], key="align2")
        if f2:
            path = os.path.join(UPLOAD_DIR, f2.name)
            with open(path, "wb") as f:
                f.write(f2.getbuffer())
            update_agent_memory({"align_file2": f2.name})
            st.info(f"ç›®æ ‡æ–‡ä»¶ï¼š{f2.name}")


# =============================================================
# ä¸»èŠå¤©åŒºåŸŸï¼šæ¸²æŸ“å†å²å¯¹è¯
# =============================================================
for i, msg in enumerate(st.session_state.messages):
    with st.chat_message("user" if isinstance(msg, HumanMessage) else "assistant"):
        render_message_content(msg.content, unique_key_prefix=f"history_{i}")

config = {"configurable": {"thread_id": st.session_state.thread_id}}
# =============================================================
# å¤„ç†ç”¨æˆ·è¾“å…¥
# =============================================================
if user_input := st.chat_input("è¯·è¾“å…¥ä½ çš„æŒ‡ä»¤â€¦"):

    # 1. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    st.chat_message("user").write(user_input)
    st.session_state.messages.append(HumanMessage(content=user_input))

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        llm_response_text = ""
        
        capturer = AggressivePrintCapture()

        try:
            snapshot = st.session_state.graph.get_state(config)
        
            inputs = None
            events = None # åˆå§‹åŒ–äº‹ä»¶ç”Ÿæˆå™¨

            # B. æ£€æŸ¥æ˜¯å¦å¤„äºâ€œæš‚åœ/ä¸­æ–­â€çŠ¶æ€
            is_paused_at_ask_user = snapshot.next and "ask_user" in snapshot.next
            
            if is_paused_at_ask_user:
                # --- åˆ†æ”¯ 1: æ¢å¤æ¨¡å¼ (Resume) ---
                # snapshot.next ä¸ä¸ºç©ºï¼Œè¯´æ˜ä¸Šæ¬¡è¿è¡Œåœ¨æŸä¸ªèŠ‚ç‚¹åœä¸‹äº†ï¼ˆæ¯”å¦‚ ask_userï¼‰
                st.toast("æ£€æµ‹åˆ°è¿›è¡Œä¸­çš„ä»»åŠ¡ï¼Œæ­£åœ¨ç»§ç»­...", icon="ğŸ”„")
                
                # 1. å°†ç”¨æˆ·çš„è¾“å…¥ï¼ˆä¾‹å¦‚ "A" æˆ– "B"ï¼‰æ³¨å…¥åˆ°çŠ¶æ€ä¸­
                # as_node="ask_user" è¡¨ç¤ºæŠŠè¿™æ¡æ¶ˆæ¯å½“ä½œæ˜¯ ask_user èŠ‚ç‚¹æ¥æ”¶åˆ°çš„åç»­è¾“å…¥
                st.session_state.graph.update_state(
                    config, 
                    {"messages": [HumanMessage(content=user_input)]},
                    as_node="ask_user"  # ğŸ‘ˆ ç¡®ä¿è¿™é‡Œè·Ÿä½ çš„å›¾ç»“æ„ä¸­äº§ç”Ÿä¸­æ–­çš„èŠ‚ç‚¹åä¸€è‡´
                )
                
                # 2. ç»§ç»­è¿è¡Œ (ä¼ å…¥ None è¡¨ç¤ºä»æ–­ç‚¹ç»§ç»­)
                # æ­¤æ—¶ mode è®¾ä¸º None æˆ–ç‰¹å®šå€¼ï¼Œé¿å…æ¸²æŸ“é”™è¯¯çš„æ€ç»´å¯¼å›¾
                inputs = None
                mode = None 

            else:
                # --- åˆ†æ”¯ 2: æ–°ä»»åŠ¡æ¨¡å¼ (New Run) ---
                # ä¹‹å‰çš„æµç¨‹å·²ç»“æŸï¼Œè¿™æ˜¯å…¨æ–°çš„è¯·æ±‚
                mode = None
                if "æ¸…æ´—" in user_input: mode = "clean"
                if "å¯¹é½" in user_input: mode = "align"
                
                # 1. æ„å»ºæ ‡å‡†è¾“å…¥
                inputs = {"messages": [HumanMessage(content=user_input)]}
                

            # 2. æ€ç»´å¯¼å›¾åŠ¨ç”»
            if mode:
                with st.expander("ğŸ“Š æ™ºèƒ½ä½“æ€ç»´è§„åˆ’ï¼ˆåŠ¨æ€ï¼‰", expanded=True):
                    render_flowchart_stepwise(st, mode)
            
            # 3. è¿‡ç¨‹æ—¥å¿—å®¹å™¨ (ä½äºæ€ç»´å¯¼å›¾ä¸‹æ–¹ï¼Œæœ€ç»ˆå›ç­”ä¸Šæ–¹)
            log_display_container = st.container()
            
            # 4. è¿è¡Œ Graph
            with st.status("ğŸ¤– AI æ­£åœ¨æ‰§è¡Œä»»åŠ¡...", expanded=True) as status_box:
                
                log_box = st.empty()
                capturer.set_placeholder(log_box)

                with capturer:
                    events = st.session_state.graph.stream(inputs, config=config)

                    for event in events:
                        for node_name, value in event.items():
                            if value is None: continue

                            render_step_details(status_box, value, node_name)

                            if isinstance(value, dict) and "messages" in value and value["messages"]:
                                last_msg = value["messages"][-1]
                                if isinstance(last_msg, AIMessage):
                                    llm_response_text = last_msg.content
                                    message_placeholder.markdown(llm_response_text + "â–Œ")
                
                status_box.update(label="âœ… å¤„ç†å®Œæˆ", state="complete", expanded=False)

            # ------------------------------------------------------------------
            # 5. ğŸ”¥ å¤„ç†æ—¥å¿—è¾“å‡º (åˆ†ç¦»æ¸²æŸ“å¹¶å»é™¤é‡å¤è¡Œ)
            # ------------------------------------------------------------------
            captured_text = capturer.get_all_logs()
            log_section_for_history = ""
            
            if captured_text.strip():
                log_lines = captured_text.strip().split('\n')
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç§»é™¤æœ€åå¯èƒ½é‡å¤çš„æ—¥å¿—è¡Œ (ä¾‹å¦‚ï¼šå¯¹é½å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°...)
                # æ£€æŸ¥æœ€åä¸€è¡Œæ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼ˆè¿™æ˜¯ LangChain å€¾å‘äºé‡å¤åˆ°æœ€ç»ˆè¾“å‡ºä¸­çš„å†…å®¹ï¼‰
                if 'å¯¹é½å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:' in log_lines[-1] and len(log_lines) > 1:
                    log_lines.pop()
                    
                captured_text_filtered = '\n'.join(log_lines) + '\n' if log_lines else ''

                # 5a. æ¸²æŸ“åˆ°ä¸´æ—¶å®¹å™¨ (æ»¡è¶³åœ¨å›ç­”ä¸Šæ–¹çš„è¦æ±‚)
                log_display_container.markdown("---")
                log_display_container.markdown("### ğŸ§¾ è¿‡ç¨‹æ—¥å¿— (Process Logs)")
                log_display_container.code(captured_text_filtered, language="text")

                # 5b. å‡†å¤‡å†å²è®°å½•çš„ä¿å­˜æ ¼å¼ (ä½¿ç”¨è¿‡æ»¤åçš„æ—¥å¿—)
                if captured_text_filtered.strip():
                    log_section_for_history = f"\n\n---\n**ğŸ§¾ è¿‡ç¨‹æ—¥å¿— (Process Logs):**\n```text\n{captured_text_filtered}\n```"
            
            # 6. ç»„åˆæœ€ç»ˆå†…å®¹ (LLMæ–‡æœ¬ + å†å²æ—¥å¿—)
            full_response_with_logs = llm_response_text + log_section_for_history

            # æ¸…ç©º streaming placeholder
            message_placeholder.empty()

            # 7. ä¿å­˜æ¶ˆæ¯åˆ° Session State
            st.session_state.messages.append(AIMessage(content=full_response_with_logs))
            
            # 8. ç«‹å³è°ƒç”¨è‡ªå®šä¹‰æ¸²æŸ“å‡½æ•°æ˜¾ç¤ºæœ€ç»ˆç»“æœå’ŒæŒ‰é’®
            render_message_content(full_response_with_logs, unique_key_prefix=f"current_{len(st.session_state.messages)}")

        except Exception as e:
            st.error(f"âŒ è¿è¡Œé”™è¯¯ï¼š{e}")
            import traceback
            st.code(traceback.format_exc())